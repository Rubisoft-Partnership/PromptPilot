version: '3.8'

services:
  cloud_sql_proxy:
    image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:${CLOUD_SQL_PROXY_VERSION}
    container_name: cloud_sql_proxy
    command: >
      --port=${CLOUD_SQL_PROXY_PORT} --address=${HOSTNAME} --credentials-file=${CLOUD_SQL_PROXY_CREDENTIALS} ${GCLOUD_CONNECTION_NAME}
    ports:
      - "${CLOUD_SQL_PROXY_PORT}:${CLOUD_SQL_PROXY_PORT}"
    volumes:
      - ${CLOUD_SQL_PROXY_CREDENTIALS_PATH}:${CLOUD_SQL_PROXY_CREDENTIALS_VOLUME}:ro
    environment:
      - RUN_AS_USER=0
    env_file:
      - .env
    networks:
      - langfuse-network

  langfuse:
    image: langfuse/langfuse:${LANGFUSE_IMAGE_VERSION}
    container_name: langfuse
    env_file:
      - .env
    ports:
      - "${LANGFUSE_PORT}:${LANGFUSE_PORT}"
    depends_on:
      - cloud_sql_proxy
    networks:
      - langfuse-network
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "node -e \"require('http').get('http://localhost:' + (process.env.LANGFUSE_PORT || 3000), res => process.exit(res.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1));\"",
        ]
      interval: 10s
      timeout: 5s
      retries: 2
      start_period: 5s
  
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ./models:/root/.ollama
    networks:
      - langfuse-network

  open_webui:
    image: ghcr.io/open-webui/open-webui:ollama
    container_name: open_webui
    ports:
      - "3001:8080"
    volumes:
      - open-webui-data:/app/backend/data
      - ./models:/root/.ollama  # Ensure access to Ollama models
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434  # Connect to the Ollama service within the network
    depends_on:
      - ollama
    networks:
      - langfuse-network
    restart: always

  application:
    build: 
      context: .
      dockerfile: app/Dockerfile
    container_name: application
    env_file:
      - .env
    depends_on:
      langfuse:
        condition: service_healthy
      ollama:
        condition: service_started
    networks:
      - langfuse-network
    ports:
      - "5001:5001"

  evaluator:
    build:
      context: ./evaluator
      dockerfile: Dockerfile
    container_name: evaluator
    env_file:
      - .env
    depends_on:
      langfuse:
        condition: service_healthy
      ollama:
        condition: service_started
    networks:
      - langfuse-network
    restart: unless-stopped

networks:
  langfuse-network:
    driver: bridge

volumes:
  open-webui-data:
    driver: local